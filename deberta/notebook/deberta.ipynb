{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7186795,
     "sourceType": "datasetVersion",
     "datasetId": 4154885
    }
   ],
   "dockerImageVersionId": 30616,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:03.857528Z",
     "iopub.execute_input": "2023-12-17T08:38:03.857847Z",
     "iopub.status.idle": "2023-12-17T08:38:05.485002Z",
     "shell.execute_reply.started": "2023-12-17T08:38:03.857817Z",
     "shell.execute_reply": "2023-12-17T08:38:05.483575Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:05.487229Z",
     "iopub.execute_input": "2023-12-17T08:38:05.487722Z",
     "iopub.status.idle": "2023-12-17T08:38:09.153121Z",
     "shell.execute_reply.started": "2023-12-17T08:38:05.487688Z",
     "shell.execute_reply": "2023-12-17T08:38:09.152052Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForTokenClassification,PretrainedConfig,AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import f1_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:09.154255Z",
     "iopub.execute_input": "2023-12-17T08:38:09.154657Z",
     "iopub.status.idle": "2023-12-17T08:38:12.288613Z",
     "shell.execute_reply.started": "2023-12-17T08:38:09.154631Z",
     "shell.execute_reply": "2023-12-17T08:38:12.287588Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device= torch.device(\"cuda\")\n",
    "device"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.290870Z",
     "iopub.execute_input": "2023-12-17T08:38:12.291407Z",
     "iopub.status.idle": "2023-12-17T08:38:12.299140Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.291376Z",
     "shell.execute_reply": "2023-12-17T08:38:12.298188Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def transform_df(df):\n",
    "    df['label'] = df['label'].apply(lambda x: [0, 1] if x == 1 else [1, 0])\n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.300422Z",
     "iopub.execute_input": "2023-12-17T08:38:12.300785Z",
     "iopub.status.idle": "2023-12-17T08:38:12.309495Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.300755Z",
     "shell.execute_reply": "2023-12-17T08:38:12.308479Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_json(\"/kaggle/input/meme-dataset/train.jsonl\",lines=True)\n",
    "test = pd.read_json(\"/kaggle/input/meme-dataset/test.jsonl\",lines=True)\n",
    "val = pd.read_json(\"/kaggle/input/meme-dataset/dev.jsonl\",lines=True)\n",
    "val.to_csv('predictions.tsv', index = False, sep = '\\t')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.310639Z",
     "iopub.execute_input": "2023-12-17T08:38:12.311002Z",
     "iopub.status.idle": "2023-12-17T08:38:12.424742Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.310976Z",
     "shell.execute_reply": "2023-12-17T08:38:12.423896Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = transform_df(data)\n",
    "val = transform_df(val)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.425993Z",
     "iopub.execute_input": "2023-12-17T08:38:12.426404Z",
     "iopub.status.idle": "2023-12-17T08:38:12.438203Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.426368Z",
     "shell.execute_reply": "2023-12-17T08:38:12.437231Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.439372Z",
     "iopub.execute_input": "2023-12-17T08:38:12.439637Z",
     "iopub.status.idle": "2023-12-17T08:38:12.460737Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.439614Z",
     "shell.execute_reply": "2023-12-17T08:38:12.459776Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "      id            img   label  \\\n0  42953  img/42953.png  [1, 0]   \n1  23058  img/23058.png  [1, 0]   \n2  13894  img/13894.png  [1, 0]   \n3  37408  img/37408.png  [1, 0]   \n4  82403  img/82403.png  [1, 0]   \n\n                                                text  \n0   its their character not their color that matters  \n1  don't be afraid to love again everyone is not ...  \n2                           putting bows on your pet  \n3  i love everything and everybody! except for sq...  \n4  everybody loves chocolate chip cookies, even h...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>42953</td>\n      <td>img/42953.png</td>\n      <td>[1, 0]</td>\n      <td>its their character not their color that matters</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23058</td>\n      <td>img/23058.png</td>\n      <td>[1, 0]</td>\n      <td>don't be afraid to love again everyone is not ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13894</td>\n      <td>img/13894.png</td>\n      <td>[1, 0]</td>\n      <td>putting bows on your pet</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37408</td>\n      <td>img/37408.png</td>\n      <td>[1, 0]</td>\n      <td>i love everything and everybody! except for sq...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82403</td>\n      <td>img/82403.png</td>\n      <td>[1, 0]</td>\n      <td>everybody loves chocolate chip cookies, even h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.462080Z",
     "iopub.execute_input": "2023-12-17T08:38:12.462526Z",
     "iopub.status.idle": "2023-12-17T08:38:12.472526Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.462489Z",
     "shell.execute_reply": "2023-12-17T08:38:12.471470Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "      id            img                                         text\n0  16395  img/16395.png                     handjobs sold seperately\n1  37405  img/37405.png         introducing fidget spinner for women\n2  94180  img/94180.png  happy pride month let's go beat up lesbians\n3  54321  img/54321.png       laughs in [majority of u.s crime rate]\n4  97015  img/97015.png       finds out those 72 virgins.. are goats",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16395</td>\n      <td>img/16395.png</td>\n      <td>handjobs sold seperately</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37405</td>\n      <td>img/37405.png</td>\n      <td>introducing fidget spinner for women</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>94180</td>\n      <td>img/94180.png</td>\n      <td>happy pride month let's go beat up lesbians</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>54321</td>\n      <td>img/54321.png</td>\n      <td>laughs in [majority of u.s crime rate]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97015</td>\n      <td>img/97015.png</td>\n      <td>finds out those 72 virgins.. are goats</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "val.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.478139Z",
     "iopub.execute_input": "2023-12-17T08:38:12.478942Z",
     "iopub.status.idle": "2023-12-17T08:38:12.491098Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.478913Z",
     "shell.execute_reply": "2023-12-17T08:38:12.490231Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "      id            img   label  \\\n0   8291  img/08291.png  [0, 1]   \n1  46971  img/46971.png  [0, 1]   \n2   3745  img/03745.png  [0, 1]   \n3  83745  img/83745.png  [0, 1]   \n4  80243  img/80243.png  [0, 1]   \n\n                                                text  \n0              white people is this a shooting range  \n1                              bravery at its finest  \n2  your order comes to $37.50 and your white priv...  \n3  it is time.. to send these parasites back to t...  \n4                             mississippi wind chime  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8291</td>\n      <td>img/08291.png</td>\n      <td>[0, 1]</td>\n      <td>white people is this a shooting range</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>46971</td>\n      <td>img/46971.png</td>\n      <td>[0, 1]</td>\n      <td>bravery at its finest</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3745</td>\n      <td>img/03745.png</td>\n      <td>[0, 1]</td>\n      <td>your order comes to $37.50 and your white priv...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>83745</td>\n      <td>img/83745.png</td>\n      <td>[0, 1]</td>\n      <td>it is time.. to send these parasites back to t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>80243</td>\n      <td>img/80243.png</td>\n      <td>[0, 1]</td>\n      <td>mississippi wind chime</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pytorch-transformers"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:12.492425Z",
     "iopub.execute_input": "2023-12-17T08:38:12.492796Z",
     "iopub.status.idle": "2023-12-17T08:38:32.469965Z",
     "shell.execute_reply.started": "2023-12-17T08:38:12.492759Z",
     "shell.execute_reply": "2023-12-17T08:38:32.468719Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting pytorch-transformers\n  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.4/176.4 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (1.24.3)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (4.66.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2023.8.8)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (0.1.99)\nCollecting sacremoses (from pytorch-transformers)\n  Obtaining dependency information for sacremoses from https://files.pythonhosted.org/packages/0b/f0/89ee2bc9da434bd78464f288fdb346bc2932f2ee80a90b2a4bbbac262c74/sacremoses-0.1.1-py3-none-any.whl.metadata\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->pytorch-transformers)\n  Obtaining dependency information for botocore<1.30.0,>=1.29.100 from https://files.pythonhosted.org/packages/46/20/e7a9a8e6746872afcc4e3ad5ab503702c38813b3a532df27cce95c98b8cb/botocore-1.29.165-py3-none-any.whl.metadata\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-transformers) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-transformers) (0.6.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (2023.11.17)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch-transformers) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch-transformers) (1.3.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch-transformers) (2.8.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch-transformers) (1.16.0)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m897.5/897.5 kB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\n\u001B[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.0/11.0 MB\u001B[0m \u001B[31m37.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: sacremoses, botocore, pytorch-transformers\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.33.1\n    Uninstalling botocore-1.33.1:\n      Successfully uninstalled botocore-1.33.1\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.8.0 requires botocore<1.33.2,>=1.32.4, but you have botocore 1.29.165 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed botocore-1.29.165 pytorch-transformers-1.2.0 sacremoses-0.1.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_transformers import RobertaModel, RobertaTokenizer\n",
    "from pytorch_transformers import RobertaForSequenceClassification, RobertaConfig"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:32.471778Z",
     "iopub.execute_input": "2023-12-17T08:38:32.472571Z",
     "iopub.status.idle": "2023-12-17T08:38:33.156715Z",
     "shell.execute_reply.started": "2023-12-17T08:38:32.472528Z",
     "shell.execute_reply": "2023-12-17T08:38:33.155941Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def data_collator(batch):\n",
    "    y = torch.Tensor([p[1] for p in batch]).to(device)\n",
    "    x = tokenizer([p[0] for p in batch], return_tensors='pt', padding='max_length',truncation=True).to(device)\n",
    "    return (x, y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.158021Z",
     "iopub.execute_input": "2023-12-17T08:38:33.158834Z",
     "iopub.status.idle": "2023-12-17T08:38:33.166302Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.158795Z",
     "shell.execute_reply": "2023-12-17T08:38:33.165191Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataloaders(train, valid):\n",
    "    train_dataset = PairsDataset(train.text.values, train.label.values)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    \n",
    "    valid_dataset = PairsDataset(valid.text.values, valid.label.values)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False, collate_fn=data_collator)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.167563Z",
     "iopub.execute_input": "2023-12-17T08:38:33.167898Z",
     "iopub.status.idle": "2023-12-17T08:38:33.178096Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.167864Z",
     "shell.execute_reply": "2023-12-17T08:38:33.177178Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "    y_pred_prob = list()\n",
    "    f1_valid = .0\n",
    "    for x, y in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=x.input_ids,\n",
    "                attention_mask=x.attention_mask,\n",
    "                labels=y,\n",
    "                return_dict=True\n",
    "            )\n",
    "            loss = output.loss\n",
    "            \n",
    "            num += len(x) * loss.item()\n",
    "            den += len(x)\n",
    "            \n",
    "            y_pred.extend(torch.argmax(output.logits, 1).tolist())\n",
    "            y_pred_prob.extend(softmax(output.logits, dim = 1)[:, 1].tolist())\n",
    "            y_true.extend(torch.argmax(y, 1).tolist())\n",
    "            \n",
    "    val_loss = num / den\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    return val_loss, acc, y_pred_prob"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.179205Z",
     "iopub.execute_input": "2023-12-17T08:38:33.179497Z",
     "iopub.status.idle": "2023-12-17T08:38:33.192983Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.179473Z",
     "shell.execute_reply": "2023-12-17T08:38:33.192096Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=10, \n",
    "    lr=1e-5,\n",
    "    eval_steps = 50\n",
    "):\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size = 3, gamma=0.5)\n",
    "    best_acc = float('-inf')\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH', epoch)\n",
    "        losses = list()\n",
    "        for i, (x, y) in enumerate(train_dataloader):            \n",
    "            output = model(\n",
    "                input_ids=x.input_ids,\n",
    "                attention_mask=x.attention_mask,\n",
    "                labels=y,\n",
    "                return_dict=True\n",
    "            )\n",
    "            loss = output.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if i % eval_steps == 0:\n",
    "                model.eval()\n",
    "                train_loss = np.mean(losses[-eval_steps:])\n",
    "                eval_loss, eval_acc, _ = evaluate_model(model, val_dataloader)\n",
    "                if eval_acc > best_acc:\n",
    "                    best_acc = eval_acc\n",
    "                    torch.save(model.state_dict(), SAVE_PATH)\n",
    "                print(f'step {i} train_loss: {train_loss:.3} eval_loss: {eval_loss:.3} eval_acc: {eval_acc:.3}')\n",
    "                model.train()\n",
    "        scheduler.step()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.194246Z",
     "iopub.execute_input": "2023-12-17T08:38:33.194584Z",
     "iopub.status.idle": "2023-12-17T08:38:33.205310Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.194554Z",
     "shell.execute_reply": "2023-12-17T08:38:33.204414Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.206515Z",
     "iopub.execute_input": "2023-12-17T08:38:33.206799Z",
     "iopub.status.idle": "2023-12-17T08:38:33.218516Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.206767Z",
     "shell.execute_reply": "2023-12-17T08:38:33.217557Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" CharacterTokenzier for Hugging Face Transformers.\n",
    "\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self._vocab_str_to_int\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.219875Z",
     "iopub.execute_input": "2023-12-17T08:38:33.220226Z",
     "iopub.status.idle": "2023-12-17T08:38:33.246216Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.220189Z",
     "shell.execute_reply": "2023-12-17T08:38:33.245283Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "import sys\n",
    "\n",
    "chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "model_max_length = 64\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.247574Z",
     "iopub.execute_input": "2023-12-17T08:38:33.248078Z",
     "iopub.status.idle": "2023-12-17T08:38:33.261198Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.248045Z",
     "shell.execute_reply": "2023-12-17T08:38:33.260327Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DebertaV2Config\n",
    "\n",
    "# config = DebertaV2Config(\n",
    "#     vocab_size=74,  # размер словаря\n",
    "#     hidden_size=128,  # размер скрытого состояния\n",
    "#     num_hidden_layers=4,  # количество слоев\n",
    "#     num_attention_heads=8,  # количество внимательных голов\n",
    "#     intermediate_size=1024,  # размер промежуточного слоя\n",
    "#     hidden_act=\"gelu\",  # функция активации\n",
    "#     hidden_dropout_prob=0.2,  # вероятность отключения для Dropout в скрытом слое\n",
    "#     attention_probs_dropout_prob=0.2,  # вероятность отключения для Dropout внимательных голов\n",
    "#     max_position_embeddings=64,\n",
    "#     max_length= 66,\n",
    "# #max_relative_positions=64,\n",
    "#     type_vocab_size=0,  # размер словаря типов\n",
    "#     pooler_dropout= 0,\n",
    "#     pooler_hidden_act=\"gelu\",\n",
    "#    pooler_hidden_size= 128,\n",
    "#    position_biased_input = True,\n",
    "#    relative_attention = True,\n",
    "#    initializer_range=0.02,\n",
    "#     num_classes=2,\n",
    "# )\n",
    "config = DebertaV2Config.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "config.num_labels = 2\n",
    "config"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.262359Z",
     "iopub.execute_input": "2023-12-17T08:38:33.262637Z",
     "iopub.status.idle": "2023-12-17T08:38:33.510441Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.262612Z",
     "shell.execute_reply": "2023-12-17T08:38:33.509486Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaecb05b53d84ba8a5a0f952aa8ad483"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DebertaV2Config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.35.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from transformers import DebertaTokenizer\n",
    "# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.511707Z",
     "iopub.execute_input": "2023-12-17T08:38:33.511994Z",
     "iopub.status.idle": "2023-12-17T08:38:33.516160Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.511969Z",
     "shell.execute_reply": "2023-12-17T08:38:33.515119Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DebertaV2ForSequenceClassification\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "SAVE_PATH = 'roberta_model'\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "test_results = list()\n",
    "\n",
    "test_dataset = PairsDataset(val.text.values, val.label.values)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "    print(f\"=====  FOLD {i}  =====\")\n",
    "    train, valid = data.iloc[train_index], data.iloc[test_index]\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(train, valid)\n",
    "    \n",
    "    model = DebertaV2ForSequenceClassification(config).to(device);\n",
    "    model.train()\n",
    "    \n",
    "    train_loop(model, train_dataloader, valid_dataloader, max_epochs=EPOCHS, lr=2e-5, eval_steps = 50)\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    model.eval()\n",
    "    \n",
    "    _, _, test_probability = evaluate_model(model, test_dataloader)\n",
    "    test_results.append(test_probability)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T08:38:33.517553Z",
     "iopub.execute_input": "2023-12-17T08:38:33.518264Z",
     "iopub.status.idle": "2023-12-17T09:27:50.518916Z",
     "shell.execute_reply.started": "2023-12-17T08:38:33.518213Z",
     "shell.execute_reply": "2023-12-17T09:27:50.518034Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "=====  FOLD 0  =====\nEPOCH 0\nstep 0 train_loss: 0.702 eval_loss: 0.719 eval_acc: 0.64\nstep 50 train_loss: 0.656 eval_loss: 0.656 eval_acc: 0.64\nstep 100 train_loss: 0.653 eval_loss: 0.657 eval_acc: 0.641\nEPOCH 1\nstep 0 train_loss: 0.617 eval_loss: 0.655 eval_acc: 0.64\nstep 50 train_loss: 0.648 eval_loss: 0.644 eval_acc: 0.633\nstep 100 train_loss: 0.646 eval_loss: 0.647 eval_acc: 0.636\nEPOCH 2\nstep 0 train_loss: 0.622 eval_loss: 0.649 eval_acc: 0.639\nstep 50 train_loss: 0.638 eval_loss: 0.644 eval_acc: 0.646\nstep 100 train_loss: 0.648 eval_loss: 0.648 eval_acc: 0.639\nEPOCH 3\nstep 0 train_loss: 0.597 eval_loss: 0.646 eval_acc: 0.638\nstep 50 train_loss: 0.624 eval_loss: 0.649 eval_acc: 0.628\nstep 100 train_loss: 0.633 eval_loss: 0.647 eval_acc: 0.639\nEPOCH 4\nstep 0 train_loss: 0.666 eval_loss: 0.657 eval_acc: 0.612\nstep 50 train_loss: 0.626 eval_loss: 0.647 eval_acc: 0.642\nstep 100 train_loss: 0.621 eval_loss: 0.668 eval_acc: 0.623\n=====  FOLD 1  =====\nEPOCH 0\nstep 0 train_loss: 0.73 eval_loss: 0.779 eval_acc: 0.651\nstep 50 train_loss: 0.667 eval_loss: 0.673 eval_acc: 0.624\nstep 100 train_loss: 0.648 eval_loss: 0.644 eval_acc: 0.651\nEPOCH 1\nstep 0 train_loss: 0.622 eval_loss: 0.651 eval_acc: 0.647\nstep 50 train_loss: 0.65 eval_loss: 0.649 eval_acc: 0.651\nstep 100 train_loss: 0.647 eval_loss: 0.643 eval_acc: 0.649\nEPOCH 2\nstep 0 train_loss: 0.568 eval_loss: 0.642 eval_acc: 0.643\nstep 50 train_loss: 0.646 eval_loss: 0.652 eval_acc: 0.627\nstep 100 train_loss: 0.646 eval_loss: 0.653 eval_acc: 0.651\nEPOCH 3\nstep 0 train_loss: 0.643 eval_loss: 0.641 eval_acc: 0.627\nstep 50 train_loss: 0.633 eval_loss: 0.644 eval_acc: 0.615\nstep 100 train_loss: 0.634 eval_loss: 0.638 eval_acc: 0.639\nEPOCH 4\nstep 0 train_loss: 0.525 eval_loss: 0.648 eval_acc: 0.651\nstep 50 train_loss: 0.638 eval_loss: 0.646 eval_acc: 0.643\nstep 100 train_loss: 0.622 eval_loss: 0.638 eval_acc: 0.649\n=====  FOLD 2  =====\nEPOCH 0\nstep 0 train_loss: 0.688 eval_loss: 0.882 eval_acc: 0.638\nstep 50 train_loss: 0.666 eval_loss: 0.661 eval_acc: 0.638\nstep 100 train_loss: 0.654 eval_loss: 0.653 eval_acc: 0.638\nEPOCH 1\nstep 0 train_loss: 0.613 eval_loss: 0.659 eval_acc: 0.638\nstep 50 train_loss: 0.648 eval_loss: 0.652 eval_acc: 0.639\nstep 100 train_loss: 0.651 eval_loss: 0.647 eval_acc: 0.638\nEPOCH 2\nstep 0 train_loss: 0.602 eval_loss: 0.645 eval_acc: 0.638\nstep 50 train_loss: 0.648 eval_loss: 0.661 eval_acc: 0.638\nstep 100 train_loss: 0.638 eval_loss: 0.647 eval_acc: 0.638\nEPOCH 3\nstep 0 train_loss: 0.559 eval_loss: 0.641 eval_acc: 0.646\nstep 50 train_loss: 0.628 eval_loss: 0.64 eval_acc: 0.661\nstep 100 train_loss: 0.635 eval_loss: 0.646 eval_acc: 0.638\nEPOCH 4\nstep 0 train_loss: 0.621 eval_loss: 0.643 eval_acc: 0.638\nstep 50 train_loss: 0.631 eval_loss: 0.641 eval_acc: 0.665\nstep 100 train_loss: 0.629 eval_loss: 0.64 eval_acc: 0.65\n=====  FOLD 3  =====\nEPOCH 0\nstep 0 train_loss: 0.687 eval_loss: 0.842 eval_acc: 0.653\nstep 50 train_loss: 0.66 eval_loss: 0.646 eval_acc: 0.653\nstep 100 train_loss: 0.658 eval_loss: 0.645 eval_acc: 0.653\nEPOCH 1\nstep 0 train_loss: 0.653 eval_loss: 0.661 eval_acc: 0.65\nstep 50 train_loss: 0.654 eval_loss: 0.651 eval_acc: 0.653\nstep 100 train_loss: 0.643 eval_loss: 0.655 eval_acc: 0.653\nEPOCH 2\nstep 0 train_loss: 0.662 eval_loss: 0.64 eval_acc: 0.652\nstep 50 train_loss: 0.643 eval_loss: 0.642 eval_acc: 0.645\nstep 100 train_loss: 0.64 eval_loss: 0.637 eval_acc: 0.646\nEPOCH 3\nstep 0 train_loss: 0.611 eval_loss: 0.644 eval_acc: 0.616\nstep 50 train_loss: 0.636 eval_loss: 0.647 eval_acc: 0.654\nstep 100 train_loss: 0.63 eval_loss: 0.64 eval_acc: 0.625\nEPOCH 4\nstep 0 train_loss: 0.644 eval_loss: 0.639 eval_acc: 0.631\nstep 50 train_loss: 0.62 eval_loss: 0.643 eval_acc: 0.64\nstep 100 train_loss: 0.636 eval_loss: 0.64 eval_acc: 0.641\n=====  FOLD 4  =====\nEPOCH 0\nstep 0 train_loss: 0.73 eval_loss: 0.793 eval_acc: 0.624\nstep 50 train_loss: 0.655 eval_loss: 0.662 eval_acc: 0.624\nstep 100 train_loss: 0.652 eval_loss: 0.662 eval_acc: 0.616\nEPOCH 1\nstep 0 train_loss: 0.613 eval_loss: 0.681 eval_acc: 0.624\nstep 50 train_loss: 0.644 eval_loss: 0.671 eval_acc: 0.609\nstep 100 train_loss: 0.634 eval_loss: 0.673 eval_acc: 0.618\nEPOCH 2\nstep 0 train_loss: 0.646 eval_loss: 0.661 eval_acc: 0.624\nstep 50 train_loss: 0.644 eval_loss: 0.661 eval_acc: 0.62\nstep 100 train_loss: 0.634 eval_loss: 0.669 eval_acc: 0.622\nEPOCH 3\nstep 0 train_loss: 0.646 eval_loss: 0.687 eval_acc: 0.556\nstep 50 train_loss: 0.629 eval_loss: 0.665 eval_acc: 0.628\nstep 100 train_loss: 0.624 eval_loss: 0.67 eval_acc: 0.625\nEPOCH 4\nstep 0 train_loss: 0.644 eval_loss: 0.658 eval_acc: 0.627\nstep 50 train_loss: 0.618 eval_loss: 0.667 eval_acc: 0.615\nstep 100 train_loss: 0.628 eval_loss: 0.674 eval_acc: 0.626\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# assert all([len(x) == len(test) for x in test_results])\n",
    "print(test_results)\n",
    "\n",
    "predictions = np.mean(test_results, axis = 0)\n",
    "val.label = [1 if x >= 0.5 else 0 for x in predictions]\n",
    "val.to_csv('predictions1.tsv', index = False, sep = '\\t')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T09:29:54.167563Z",
     "iopub.execute_input": "2023-12-17T09:29:54.167986Z",
     "iopub.status.idle": "2023-12-17T09:29:54.183528Z",
     "shell.execute_reply.started": "2023-12-17T09:29:54.167953Z",
     "shell.execute_reply": "2023-12-17T09:29:54.182413Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "[[0.3428903818130493, 0.29686805605888367, 0.36432650685310364, 0.3688851296901703, 0.41364631056785583, 0.3778071701526642, 0.3678322732448578, 0.2852848768234253, 0.20994886755943298, 0.2908725440502167, 0.4123210310935974, 0.43214505910873413, 0.3379700183868408, 0.46204859018325806, 0.37785500288009644, 0.40267282724380493, 0.27828964591026306, 0.508456289768219, 0.35436195135116577, 0.3758874237537384, 0.4388878643512726, 0.28471603989601135, 0.5225841999053955, 0.42313703894615173, 0.38374975323677063, 0.4060947597026825, 0.45746028423309326, 0.334932416677475, 0.45861420035362244, 0.4873514473438263, 0.32072359323501587, 0.24260541796684265, 0.27572184801101685, 0.28555384278297424, 0.3611781597137451, 0.3891407251358032, 0.40139704942703247, 0.5138797760009766, 0.3052823841571808, 0.5780176520347595, 0.5279096364974976, 0.5063440203666687, 0.29834866523742676, 0.39084023237228394, 0.4266846776008606, 0.5660996437072754, 0.2582542598247528, 0.3041543662548065, 0.4448445439338684, 0.30090466141700745, 0.40795058012008667, 0.3579358458518982, 0.32450559735298157, 0.2814646065235138, 0.41388261318206787, 0.46691951155662537, 0.3495171368122101, 0.29079994559288025, 0.38911035656929016, 0.38023829460144043, 0.24260541796684265, 0.482829213142395, 0.2575908303260803, 0.310543954372406, 0.3701040744781494, 0.4927107095718384, 0.40714195370674133, 0.46691951155662537, 0.38288408517837524, 0.32558611035346985, 0.4126097559928894, 0.3042565882205963, 0.32698628306388855, 0.43045568466186523, 0.3631485402584076, 0.2763233482837677, 0.3212524950504303, 0.35067078471183777, 0.3312241733074188, 0.3074387013912201, 0.280403196811676, 0.2804398834705353, 0.4652291536331177, 0.33367016911506653, 0.3366740345954895, 0.3577619791030884, 0.24260541796684265, 0.33692264556884766, 0.3836256265640259, 0.25111034512519836, 0.2763233482837677, 0.3042745292186737, 0.4133373200893402, 0.29818230867385864, 0.2470935583114624, 0.31630319356918335, 0.3701040744781494, 0.4013610780239105, 0.2513740658760071, 0.4240666329860687, 0.28684183955192566, 0.29692286252975464, 0.2805922329425812, 0.44208279252052307, 0.3074387013912201, 0.4659656286239624, 0.3267468512058258, 0.33367016911506653, 0.22721350193023682, 0.3855430483818054, 0.43045568466186523, 0.2645948529243469, 0.39220133423805237, 0.44362619519233704, 0.26502108573913574, 0.41915759444236755, 0.3081979751586914, 0.34859591722488403, 0.3584560453891754, 0.46164175868034363, 0.3715193271636963, 0.4631650149822235, 0.2715829312801361, 0.3506866991519928, 0.4426863193511963, 0.3184252083301544, 0.34651076793670654, 0.49752089381217957, 0.2811547517776489, 0.5414795875549316, 0.34767499566078186, 0.42142340540885925, 0.42644399404525757, 0.37232398986816406, 0.30943170189857483, 0.36293187737464905, 0.4487699866294861, 0.31367936730384827, 0.3341805040836334, 0.3341887891292572, 0.4063930809497833, 0.3612493574619293, 0.37357297539711, 0.3758366107940674, 0.3953920602798462, 0.2661997973918915, 0.46386587619781494, 0.3165692090988159, 0.33726513385772705, 0.4143792390823364, 0.4187175929546356, 0.28497549891471863, 0.31503522396087646, 0.2805922329425812, 0.25208890438079834, 0.3865278363227844, 0.3429218530654907, 0.44208279252052307, 0.4484754800796509, 0.37318623065948486, 0.5320534110069275, 0.379707008600235, 0.645145058631897, 0.2715829312801361, 0.3118363618850708, 0.3340814411640167, 0.583621621131897, 0.4925653338432312, 0.3198277950286865, 0.40338778495788574, 0.40874436497688293, 0.3528560996055603, 0.27635458111763, 0.39950382709503174, 0.23995208740234375, 0.5596587061882019, 0.27421829104423523, 0.47776928544044495, 0.2827838361263275, 0.43740496039390564, 0.35120660066604614, 0.46779605746269226, 0.2961743175983429, 0.3895326852798462, 0.319600373506546, 0.30107033252716064, 0.2505485713481903, 0.46956369280815125, 0.37232398986816406, 0.3552802801132202, 0.35131973028182983, 0.47428783774375916, 0.38364094495773315, 0.4187175929546356, 0.30658748745918274, 0.2467183768749237, 0.2905865013599396, 0.4265899360179901, 0.3476453721523285, 0.410241961479187, 0.43821409344673157, 0.309184193611145, 0.24447625875473022, 0.32698628306388855, 0.2945402264595032, 0.27819371223449707, 0.32824137806892395, 0.5099487900733948, 0.41923046112060547, 0.43673819303512573, 0.30673864483833313, 0.4471316933631897, 0.5292867422103882, 0.2944733500480652, 0.303399920463562, 0.29076462984085083, 0.288432776927948, 0.4293331503868103, 0.3093004524707794, 0.43540000915527344, 0.4126524329185486, 0.4864636957645416, 0.3581089377403259, 0.4271608293056488, 0.3520733714103699, 0.34092429280281067, 0.31012576818466187, 0.41054099798202515, 0.3108923137187958, 0.21292227506637573, 0.4998629689216614, 0.4401264786720276, 0.2505485713481903, 0.319600373506546, 0.4659656286239624, 0.33107781410217285, 0.303399920463562, 0.4682321846485138, 0.38564449548721313, 0.33061859011650085, 0.5101312398910522, 0.45070046186447144, 0.2804398834705353, 0.4325183928012848, 0.3404335379600525, 0.3268914520740509, 0.34797850251197815, 0.42282959818840027, 0.3590986728668213, 0.2709926664829254, 0.35401979088783264, 0.28946682810783386, 0.3198277950286865, 0.32066693902015686, 0.2636198401451111, 0.5016784071922302, 0.24808907508850098, 0.31906166672706604, 0.42607104778289795, 0.3091883361339569, 0.3363422453403473, 0.3082599937915802, 0.3607272505760193, 0.42800429463386536, 0.28590115904808044, 0.3226501941680908, 0.41598618030548096, 0.34767499566078186, 0.4774428606033325, 0.4484754800796509, 0.40941378474235535, 0.3752578794956207, 0.3758366107940674, 0.29857975244522095, 0.5624067187309265, 0.37357297539711, 0.27635458111763, 0.4942391812801361, 0.25667861104011536, 0.35695865750312805, 0.3632284998893738, 0.26297706365585327, 0.41169512271881104, 0.387951523065567, 0.41313794255256653, 0.34309542179107666, 0.26439031958580017, 0.31278499960899353, 0.3617505133152008, 0.40707454085350037, 0.39627546072006226, 0.31886833906173706, 0.4296550452709198, 0.30867499113082886, 0.32837116718292236, 0.33584123849868774, 0.33277615904808044, 0.3572198748588562, 0.29752492904663086, 0.33277615904808044, 0.36006665229797363, 0.33162835240364075, 0.442827433347702, 0.2954026758670807, 0.25118592381477356, 0.27984902262687683, 0.5190334916114807, 0.3679713010787964, 0.32860568165779114, 0.5541115403175354, 0.5171642303466797, 0.3201843500137329, 0.47025346755981445, 0.3283645212650299, 0.26333969831466675, 0.3153373599052429, 0.4516882598400116, 0.3165813386440277, 0.5089865922927856, 0.3479030430316925, 0.3131372034549713, 0.4567074477672577, 0.3060382306575775, 0.5409138202667236, 0.36569681763648987, 0.3893657624721527, 0.32837116718292236, 0.3316391706466675, 0.2946384847164154, 0.32682302594184875, 0.3856889009475708, 0.5171642303466797, 0.41169512271881104, 0.4941835403442383, 0.24962802231311798, 0.32874205708503723, 0.3447493016719818, 0.3151284158229828, 0.3287934958934784, 0.3314935863018036, 0.3127319812774658, 0.6290469765663147, 0.3673848509788513, 0.3201843500137329, 0.31303444504737854, 0.27200156450271606, 0.24515464901924133, 0.35643547773361206, 0.31304439902305603, 0.3014408051967621, 0.5702887773513794, 0.34553971886634827, 0.3707829415798187, 0.26201823353767395, 0.41069498658180237, 0.36758896708488464, 0.287197083234787, 0.5238234996795654, 0.4334724545478821, 0.3726993203163147, 0.40707454085350037, 0.4254901707172394, 0.5541115403175354, 0.3236124813556671, 0.4212205410003662, 0.6028389930725098, 0.3389137387275696, 0.4010993540287018, 0.29247140884399414, 0.47610798478126526, 0.3287934958934784, 0.3131372034549713, 0.3548571467399597, 0.5089865922927856, 0.4334724545478821, 0.3078063428401947, 0.3078063428401947, 0.476574182510376, 0.3919137716293335, 0.22974056005477905, 0.5409138202667236, 0.3856889009475708, 0.3165813386440277, 0.2962556481361389, 0.3572198748588562, 0.4345591068267822, 0.38558119535446167, 0.31886833906173706, 0.35643547773361206, 0.40982845425605774, 0.2640320658683777, 0.5093294978141785, 0.39627546072006226, 0.2815473973751068, 0.40700939297676086, 0.2944001257419586, 0.34379929304122925, 0.39787131547927856, 0.38558119535446167, 0.5238234996795654, 0.29521799087524414, 0.3919137716293335, 0.3374128043651581, 0.25397437810897827, 0.4979533851146698, 0.3937824070453644, 0.32682302594184875, 0.3548571467399597, 0.4861880838871002, 0.32404637336730957, 0.32011809945106506, 0.3151284158229828, 0.2505485713481903, 0.4021933376789093, 0.2944001257419586, 0.33487066626548767, 0.4516882598400116, 0.32479703426361084, 0.2861393988132477, 0.26201823353767395, 0.3720571994781494, 0.2752053439617157, 0.49472305178642273, 0.287197083234787, 0.2878057658672333, 0.4562346935272217, 0.4034760892391205, 0.4026123881340027, 0.30065208673477173, 0.40435755252838135, 0.3893657624721527, 0.4212205410003662, 0.4723545014858246, 0.46995261311531067, 0.34379929304122925, 0.29247140884399414, 0.435040146112442, 0.31947845220565796, 0.5165299773216248, 0.3479030430316925, 0.3639221489429474, 0.5161542296409607, 0.45339909195899963, 0.2752053439617157, 0.5702887773513794, 0.303311288356781, 0.36006665229797363, 0.5163028240203857, 0.3913632035255432, 0.3060382306575775, 0.4026123881340027, 0.3290866017341614, 0.35305526852607727, 0.4117039144039154, 0.3316391706466675, 0.387951523065567, 0.3758315443992615, 0.3023160994052887, 0.3952162265777588, 0.3314935863018036, 0.3076678514480591, 0.3324050009250641, 0.3679713010787964, 0.34010636806488037, 0.3447493016719818, 0.31947845220565796, 0.42405036091804504, 0.546471357345581, 0.39787131547927856, 0.303311288356781, 0.2505485713481903, 0.5144125819206238, 0.4968717694282532, 0.2867306172847748, 0.39278551936149597, 0.40435755252838135, 0.5236135721206665, 0.3937824070453644, 0.3294900953769684, 0.26000672578811646, 0.36569681763648987, 0.41810348629951477, 0.4343428313732147, 0.29327860474586487, 0.35305526852607727, 0.2646437883377075, 0.4066987931728363, 0.32860568165779114, 0.5236135721206665, 0.4723545014858246, 0.2646437883377075, 0.5161542296409607, 0.28316426277160645, 0.4296550452709198, 0.35997846722602844, 0.3359575867652893, 0.3007603883743286, 0.5163028240203857, 0.24095624685287476, 0.4936527907848358], [0.1322149634361267, 0.19554150104522705, 0.17258977890014648, 0.1614631563425064, 0.18304021656513214, 0.14210093021392822, 0.1621747761964798, 0.17690859735012054, 0.34295380115509033, 0.1614038050174713, 0.18384388089179993, 0.15016548335552216, 0.13986819982528687, 0.15715771913528442, 0.14700189232826233, 0.16413235664367676, 0.1448969841003418, 0.16389144957065582, 0.14831098914146423, 0.2170376479625702, 0.17164140939712524, 0.1690998524427414, 0.14601221680641174, 0.15268771350383759, 0.15427114069461823, 0.13927118480205536, 0.14776717126369476, 0.17150484025478363, 0.1542477160692215, 0.14838460087776184, 0.13925382494926453, 0.14931917190551758, 0.147709459066391, 0.1422761231660843, 0.203741192817688, 0.1406259983778, 0.14844033122062683, 0.16549311578273773, 0.16011999547481537, 0.16390714049339294, 0.14530766010284424, 0.15189076960086823, 0.13701355457305908, 0.16464586555957794, 0.17464607954025269, 0.1747964322566986, 0.163921058177948, 0.1301652193069458, 0.152262344956398, 0.15051274001598358, 0.13379384577274323, 0.13339626789093018, 0.1433759182691574, 0.2116793543100357, 0.15347334742546082, 0.15440401434898376, 0.14458651840686798, 0.16618655622005463, 0.1618456393480301, 0.14738479256629944, 0.14931917190551758, 0.16416262090206146, 0.1631806343793869, 0.1427043378353119, 0.14558729529380798, 0.16785036027431488, 0.15380817651748657, 0.15440401434898376, 0.154516339302063, 0.13898594677448273, 0.17062267661094666, 0.16075797379016876, 0.163046196103096, 0.18052257597446442, 0.15646818280220032, 0.14615045487880707, 0.18594156205654144, 0.13882863521575928, 0.16459256410598755, 0.15695543587207794, 0.1998271346092224, 0.15997932851314545, 0.14702293276786804, 0.1695643663406372, 0.1540997326374054, 0.14108778536319733, 0.14931917190551758, 0.2270190268754959, 0.1457793116569519, 0.14499518275260925, 0.14615045487880707, 0.13999201357364655, 0.1737995147705078, 0.14790648221969604, 0.18034188449382782, 0.1621570587158203, 0.14558729529380798, 0.17698994278907776, 0.15265126526355743, 0.15785038471221924, 0.14804145693778992, 0.19649232923984528, 0.21104444563388824, 0.1575859636068344, 0.15695543587207794, 0.14223739504814148, 0.15468309819698334, 0.1695643663406372, 0.20398476719856262, 0.1611405909061432, 0.18052257597446442, 0.17802822589874268, 0.16334004700183868, 0.1358564794063568, 0.18832795321941376, 0.14113835990428925, 0.1550419181585312, 0.1387244462966919, 0.168568417429924, 0.15495158731937408, 0.1425875574350357, 0.1582423597574234, 0.1856318861246109, 0.1689668744802475, 0.13300904631614685, 0.14246751368045807, 0.13489195704460144, 0.17507462203502655, 0.16897831857204437, 0.17406333982944489, 0.15375284850597382, 0.14523997902870178, 0.13161982595920563, 0.19424597918987274, 0.15945513546466827, 0.1455654352903366, 0.18407905101776123, 0.13582292199134827, 0.17175154387950897, 0.1475728303194046, 0.16191726922988892, 0.15527743101119995, 0.1678960919380188, 0.1493581086397171, 0.15078884363174438, 0.17275825142860413, 0.16581246256828308, 0.15372325479984283, 0.1464863121509552, 0.139789417386055, 0.1708880215883255, 0.1686152219772339, 0.13420405983924866, 0.21104444563388824, 0.15670311450958252, 0.1532953977584839, 0.13785231113433838, 0.1575859636068344, 0.14324596524238586, 0.17214025557041168, 0.16093377768993378, 0.14693480730056763, 0.15228097140789032, 0.1856318861246109, 0.14934545755386353, 0.17345266044139862, 0.1491149365901947, 0.15141087770462036, 0.13418881595134735, 0.16960282623767853, 0.18466441333293915, 0.15363937616348267, 0.20888762176036835, 0.17380250990390778, 0.1695190966129303, 0.18551833927631378, 0.17246879637241364, 0.15938512980937958, 0.1943478286266327, 0.14845877885818481, 0.14309294521808624, 0.15559172630310059, 0.1404491811990738, 0.17133203148841858, 0.178155779838562, 0.1971876323223114, 0.3098396062850952, 0.15911345183849335, 0.19424597918987274, 0.1713668256998062, 0.13902924954891205, 0.17871347069740295, 0.14726915955543518, 0.1708880215883255, 0.15605998039245605, 0.20213398337364197, 0.14522327482700348, 0.138568714261055, 0.17104367911815643, 0.13170447945594788, 0.19547221064567566, 0.156078040599823, 0.17694935202598572, 0.163046196103096, 0.13793142139911652, 0.15467320382595062, 0.15299652516841888, 0.15367136895656586, 0.18029999732971191, 0.14997363090515137, 0.15001465380191803, 0.17897231876850128, 0.16162419319152832, 0.1489771604537964, 0.15683984756469727, 0.1869947612285614, 0.17486776411533356, 0.1582990139722824, 0.1503591686487198, 0.14160077273845673, 0.16546805202960968, 0.18270590901374817, 0.1494099348783493, 0.16005665063858032, 0.16431908309459686, 0.17197494208812714, 0.1550084501504898, 0.1421051323413849, 0.15227152407169342, 0.20590698719024658, 0.1476740688085556, 0.15217113494873047, 0.3098396062850952, 0.178155779838562, 0.14223739504814148, 0.19436250627040863, 0.15683984756469727, 0.1359621286392212, 0.15852463245391846, 0.1463472843170166, 0.1480807214975357, 0.1403852105140686, 0.15997932851314545, 0.1399841010570526, 0.1565103679895401, 0.15538302063941956, 0.16699212789535522, 0.1850278079509735, 0.1487039476633072, 0.14095090329647064, 0.1330106407403946, 0.15911220014095306, 0.13418881595134735, 0.12367086857557297, 0.18972399830818176, 0.15291480720043182, 0.16377826035022736, 0.1468602865934372, 0.134525328874588, 0.15364205837249756, 0.1467321217060089, 0.15890583395957947, 0.17892110347747803, 0.16537508368492126, 0.16638632118701935, 0.15299732983112335, 0.12809790670871735, 0.15375284850597382, 0.16672486066818237, 0.14324596524238586, 0.1725137084722519, 0.14487224817276, 0.1493581086397171, 0.17442244291305542, 0.15692739188671112, 0.1678960919380188, 0.20888762176036835, 0.14922723174095154, 0.14571623504161835, 0.13015392422676086, 0.15201468765735626, 0.1528978943824768, 0.14484982192516327, 0.14770394563674927, 0.16654455661773682, 0.17858025431632996, 0.15501071512699127, 0.17806695401668549, 0.16692982614040375, 0.14150241017341614, 0.16244624555110931, 0.1416645050048828, 0.14536403119564056, 0.14323392510414124, 0.15091346204280853, 0.1500808149576187, 0.15796300768852234, 0.17718566954135895, 0.13422009348869324, 0.15796300768852234, 0.15457376837730408, 0.14484530687332153, 0.14189311861991882, 0.15779802203178406, 0.159210667014122, 0.1612427979707718, 0.16644953191280365, 0.1350328028202057, 0.17668059468269348, 0.15885712206363678, 0.1559312343597412, 0.16560032963752747, 0.15568314492702484, 0.18343190848827362, 0.176471546292305, 0.12990467250347137, 0.15406085550785065, 0.19263355433940887, 0.15295183658599854, 0.18488465249538422, 0.16889645159244537, 0.15768501162528992, 0.15123005211353302, 0.16568788886070251, 0.16035550832748413, 0.17889772355556488, 0.15091346204280853, 0.16239818930625916, 0.1624549776315689, 0.15784047544002533, 0.1637745052576065, 0.1559312343597412, 0.14484982192516327, 0.1551963984966278, 0.17080062627792358, 0.1509423851966858, 0.1458054631948471, 0.14660942554473877, 0.1667330414056778, 0.1576700210571289, 0.1814955174922943, 0.18083663284778595, 0.15016740560531616, 0.16560032963752747, 0.16304384171962738, 0.19771111011505127, 0.22796277701854706, 0.1569194495677948, 0.1323624700307846, 0.15522454679012299, 0.1551707535982132, 0.1670009046792984, 0.14059758186340332, 0.15565945208072662, 0.14908477663993835, 0.14783689379692078, 0.15345601737499237, 0.14925776422023773, 0.16455818712711334, 0.13789667189121246, 0.14150241017341614, 0.14207442104816437, 0.15885712206363678, 0.14776434004306793, 0.1576945036649704, 0.1636047661304474, 0.147659033536911, 0.1408362090587616, 0.18308083713054657, 0.16114932298660278, 0.1667330414056778, 0.16889645159244537, 0.14384421706199646, 0.15295183658599854, 0.16455818712711334, 0.14937566220760345, 0.14937566220760345, 0.16330929100513458, 0.14698556065559387, 0.22482110559940338, 0.16568788886070251, 0.1637745052576065, 0.19263355433940887, 0.16767938435077667, 0.17718566954135895, 0.15802541375160217, 0.14693127572536469, 0.1416645050048828, 0.1569194495677948, 0.15526100993156433, 0.18722394108772278, 0.1668798327445984, 0.16244624555110931, 0.15315721929073334, 0.15782056748867035, 0.19555658102035522, 0.18570147454738617, 0.1539372205734253, 0.14693127572536469, 0.14925776422023773, 0.1421853005886078, 0.14698556065559387, 0.15276141464710236, 0.16550594568252563, 0.14749866724014282, 0.14058126509189606, 0.15784047544002533, 0.14384421706199646, 0.14116457104682922, 0.1575677990913391, 0.14274267852306366, 0.14660942554473877, 0.3098396062850952, 0.15992674231529236, 0.19555658102035522, 0.16336487233638763, 0.15406085550785065, 0.20344580709934235, 0.1361587941646576, 0.15565945208072662, 0.15508893132209778, 0.13977594673633575, 0.14960139989852905, 0.15345601737499237, 0.13371509313583374, 0.14517466723918915, 0.16213206946849823, 0.15574894845485687, 0.13883328437805176, 0.1574641764163971, 0.17889772355556488, 0.1576945036649704, 0.15193605422973633, 0.15654750168323517, 0.18570147454738617, 0.18308083713054657, 0.13947737216949463, 0.14717766642570496, 0.13784284889698029, 0.18488465249538422, 0.13746236264705658, 0.17675377428531647, 0.1568043828010559, 0.13977594673633575, 0.1551707535982132, 0.15291672945022583, 0.15457376837730408, 0.1434462070465088, 0.15732985734939575, 0.15123005211353302, 0.15574894845485687, 0.17441412806510925, 0.20931552350521088, 0.15888702869415283, 0.16239818930625916, 0.14770394563674927, 0.1537047177553177, 0.20942680537700653, 0.15346994996070862, 0.1576700210571289, 0.13626283407211304, 0.17668981850147247, 0.1350328028202057, 0.15273599326610565, 0.1458054631948471, 0.14717766642570496, 0.13680516183376312, 0.17301851511001587, 0.1539372205734253, 0.15291672945022583, 0.3098396062850952, 0.15038391947746277, 0.16548173129558563, 0.1871022880077362, 0.19406957924365997, 0.1574641764163971, 0.1505916267633438, 0.14058126509189606, 0.16238459944725037, 0.17442987859249115, 0.16035550832748413, 0.14613844454288483, 0.1424088031053543, 0.13643324375152588, 0.20931552350521088, 0.1852458417415619, 0.16214214265346527, 0.17668059468269348, 0.1505916267633438, 0.15193605422973633, 0.1852458417415619, 0.17675377428531647, 0.14147046208381653, 0.14536403119564056, 0.1378764510154724, 0.15497063100337982, 0.1718023717403412, 0.1434462070465088, 0.20837175846099854, 0.15291836857795715], [0.3651876747608185, 0.21490564942359924, 0.4030061662197113, 0.3574380576610565, 0.4732397198677063, 0.39135414361953735, 0.3324144184589386, 0.29141339659690857, 0.11084716767072678, 0.2995458245277405, 0.4448793828487396, 0.4097038805484772, 0.3713664412498474, 0.4805256128311157, 0.43543165922164917, 0.4027769863605499, 0.27206769585609436, 0.5244959592819214, 0.3874078094959259, 0.25522860884666443, 0.30202409625053406, 0.3533128798007965, 0.521982729434967, 0.43062061071395874, 0.4614145755767822, 0.4463946223258972, 0.4678284525871277, 0.3940659463405609, 0.5026894807815552, 0.49593687057495117, 0.39790961146354675, 0.2557583153247833, 0.19985421001911163, 0.3965764343738556, 0.32599663734436035, 0.42033153772354126, 0.47257477045059204, 0.49365267157554626, 0.277954638004303, 0.5642725825309753, 0.5270220041275024, 0.529853105545044, 0.2913399934768677, 0.40293678641319275, 0.30288153886795044, 0.5222784280776978, 0.15916286408901215, 0.30992138385772705, 0.2766092121601105, 0.3120439350605011, 0.43959662318229675, 0.41461658477783203, 0.38966992497444153, 0.2881462574005127, 0.4258694052696228, 0.38111892342567444, 0.4041440784931183, 0.3254493474960327, 0.4128260016441345, 0.4569779634475708, 0.2557583153247833, 0.4740101993083954, 0.15970906615257263, 0.25771552324295044, 0.3558104932308197, 0.5039036870002747, 0.43293297290802, 0.38111892342567444, 0.39196860790252686, 0.45146623253822327, 0.4960298538208008, 0.3514758050441742, 0.26592594385147095, 0.4030541777610779, 0.4017384946346283, 0.16215819120407104, 0.2539411187171936, 0.3746867775917053, 0.3431316912174225, 0.2579890489578247, 0.14386995136737823, 0.19755691289901733, 0.4757506251335144, 0.3261007070541382, 0.38602736592292786, 0.466562956571579, 0.2557583153247833, 0.2759649455547333, 0.4527246654033661, 0.1302528828382492, 0.16215819120407104, 0.3573632538318634, 0.40600988268852234, 0.3602575659751892, 0.17243026196956635, 0.19272290170192719, 0.3558104932308197, 0.6109444499015808, 0.11450637131929398, 0.4638877809047699, 0.2803850471973419, 0.22960416972637177, 0.22618041932582855, 0.48112088441848755, 0.2579890489578247, 0.475933313369751, 0.39211538434028625, 0.3261007070541382, 0.2212611585855484, 0.40313079953193665, 0.4030541777610779, 0.21008767187595367, 0.45145168900489807, 0.43032532930374146, 0.2647343873977661, 0.46129611134529114, 0.41540899872779846, 0.4327632784843445, 0.3269115388393402, 0.44356799125671387, 0.3888162076473236, 0.48292702436447144, 0.20000234246253967, 0.338980108499527, 0.5165817737579346, 0.42294588685035706, 0.4154050052165985, 0.47871291637420654, 0.3123176693916321, 0.5378141403198242, 0.3988964855670929, 0.46802040934562683, 0.49494460225105286, 0.43719884753227234, 0.3408106863498688, 0.38502857089042664, 0.4278486371040344, 0.3900434970855713, 0.22018995881080627, 0.40070614218711853, 0.3999972343444824, 0.36690554022789, 0.4083299934864044, 0.38230612874031067, 0.43943971395492554, 0.19957883656024933, 0.4678868353366852, 0.29690471291542053, 0.45015856623649597, 0.49819037318229675, 0.43239036202430725, 0.25958251953125, 0.4239051043987274, 0.22618041932582855, 0.2140161097049713, 0.38772743940353394, 0.31214845180511475, 0.48112088441848755, 0.5072441101074219, 0.3578765094280243, 0.5110230445861816, 0.49863284826278687, 0.5942351818084717, 0.20000234246253967, 0.26789072155952454, 0.387990266084671, 0.5311247706413269, 0.5259062647819519, 0.4178265929222107, 0.45253297686576843, 0.4145447909832001, 0.37625640630722046, 0.21371760964393616, 0.430059015750885, 0.15295398235321045, 0.4524425268173218, 0.2874619662761688, 0.47635418176651, 0.319852352142334, 0.4692919850349426, 0.38782453536987305, 0.45246171951293945, 0.39301618933677673, 0.4613285958766937, 0.32782799005508423, 0.290310263633728, 0.23859629034996033, 0.47353464365005493, 0.43719884753227234, 0.39624184370040894, 0.42656436562538147, 0.45702064037323, 0.45964956283569336, 0.43239036202430725, 0.2971042990684509, 0.16873817145824432, 0.3031597435474396, 0.4803277552127838, 0.3991866707801819, 0.4171222746372223, 0.41751089692115784, 0.28570160269737244, 0.12017185240983963, 0.26592594385147095, 0.30939364433288574, 0.306244432926178, 0.420981228351593, 0.48950573801994324, 0.37623974680900574, 0.4843596816062927, 0.25791433453559875, 0.48623523116111755, 0.4955633878707886, 0.14077870547771454, 0.25811487436294556, 0.2252865582704544, 0.19042430818080902, 0.44132035970687866, 0.29005706310272217, 0.47964757680892944, 0.4343745708465576, 0.4555889070034027, 0.419374018907547, 0.5137653946876526, 0.44104400277137756, 0.3131641447544098, 0.3098014295101166, 0.39712512493133545, 0.2946004867553711, 0.10888414084911346, 0.446557879447937, 0.4660014808177948, 0.23859629034996033, 0.32782799005508423, 0.475933313369751, 0.39902886748313904, 0.25811487436294556, 0.5404843091964722, 0.4716769754886627, 0.2997613847255707, 0.5127307176589966, 0.5386642217636108, 0.19755691289901733, 0.4883691370487213, 0.42795082926750183, 0.31458914279937744, 0.36431828141212463, 0.40938642621040344, 0.41387084126472473, 0.29992976784706116, 0.38441169261932373, 0.22484813630580902, 0.4178265929222107, 0.37183430790901184, 0.12227681279182434, 0.5186851024627686, 0.15370632708072662, 0.44024181365966797, 0.44597122073173523, 0.3050190508365631, 0.4246337413787842, 0.4106764793395996, 0.3902605473995209, 0.47956112027168274, 0.17997688055038452, 0.35370174050331116, 0.49830982089042664, 0.3988964855670929, 0.48136451840400696, 0.5072441101074219, 0.4422800540924072, 0.43322449922561646, 0.38230612874031067, 0.2040753960609436, 0.5442647337913513, 0.4083299934864044, 0.21371760964393616, 0.49524497985839844, 0.15187294781208038, 0.45483580231666565, 0.41321316361427307, 0.17971619963645935, 0.40910354256629944, 0.39771372079849243, 0.41334617137908936, 0.3583924174308777, 0.1617312878370285, 0.2014561891555786, 0.39790651202201843, 0.40875300765037537, 0.3496231138706207, 0.38416221737861633, 0.46161240339279175, 0.2396240383386612, 0.3432336747646332, 0.37677398324012756, 0.3205670714378357, 0.4138903021812439, 0.3314990997314453, 0.3205670714378357, 0.25019219517707825, 0.3303392231464386, 0.4398062825202942, 0.4399360716342926, 0.1336505115032196, 0.19777044653892517, 0.48539453744888306, 0.4381335973739624, 0.4711713492870331, 0.5222275257110596, 0.49271586537361145, 0.25688374042510986, 0.4525132477283478, 0.2810126543045044, 0.17372117936611176, 0.3562292158603668, 0.5018543004989624, 0.25595754384994507, 0.4939212501049042, 0.4770215153694153, 0.23835986852645874, 0.38128605484962463, 0.40285301208496094, 0.46097439527511597, 0.4044314920902252, 0.4515869915485382, 0.3432336747646332, 0.3358672261238098, 0.21240288019180298, 0.37418660521507263, 0.45551010966300964, 0.49271586537361145, 0.40910354256629944, 0.4567069411277771, 0.22301319241523743, 0.3340238630771637, 0.4177491366863251, 0.24489818513393402, 0.35041022300720215, 0.37471190094947815, 0.24124225974082947, 0.5750105977058411, 0.38209038972854614, 0.25688374042510986, 0.32836341857910156, 0.25630632042884827, 0.1637234091758728, 0.44473204016685486, 0.27812567353248596, 0.3291519582271576, 0.519969642162323, 0.28868764638900757, 0.45973387360572815, 0.2884173095226288, 0.39011451601982117, 0.39128246903419495, 0.37844642996788025, 0.5144405364990234, 0.41316330432891846, 0.40870893001556396, 0.40875300765037537, 0.45209622383117676, 0.5222275257110596, 0.3931806683540344, 0.4143493175506592, 0.5364198684692383, 0.4403562545776367, 0.35114094614982605, 0.21820873022079468, 0.3978586196899414, 0.35041022300720215, 0.23835986852645874, 0.3689998984336853, 0.4939212501049042, 0.41316330432891846, 0.31781312823295593, 0.31781312823295593, 0.42938560247421265, 0.4630975127220154, 0.13121123611927032, 0.46097439527511597, 0.45551010966300964, 0.25595754384994507, 0.23609836399555206, 0.4138903021812439, 0.4243890643119812, 0.44293448328971863, 0.38416221737861633, 0.44473204016685486, 0.3757655918598175, 0.21208295226097107, 0.4620261788368225, 0.3496231138706207, 0.3340618908405304, 0.37245142459869385, 0.24113374948501587, 0.3108762204647064, 0.41431567072868347, 0.44293448328971863, 0.5144405364990234, 0.29382410645484924, 0.4630975127220154, 0.2984132766723633, 0.22717241942882538, 0.5026788711547852, 0.38718122243881226, 0.37418660521507263, 0.3689998984336853, 0.5359557271003723, 0.3270880877971649, 0.4115067720413208, 0.24489818513393402, 0.23859629034996033, 0.4077445864677429, 0.24113374948501587, 0.3532591462135315, 0.5018543004989624, 0.33561012148857117, 0.2250705510377884, 0.2884173095226288, 0.39169472455978394, 0.3150957524776459, 0.512220025062561, 0.37844642996788025, 0.20331238210201263, 0.4843382239341736, 0.4386875033378601, 0.4246756136417389, 0.31076353788375854, 0.34304240345954895, 0.4515869915485382, 0.4143493175506592, 0.4676802456378937, 0.3929334282875061, 0.3108762204647064, 0.21820873022079468, 0.4546234905719757, 0.47006016969680786, 0.5459829568862915, 0.4770215153694153, 0.4372411072254181, 0.48400798439979553, 0.46046900749206543, 0.3150957524776459, 0.519969642162323, 0.23962372541427612, 0.25019219517707825, 0.517052412033081, 0.4031541645526886, 0.40285301208496094, 0.4246756136417389, 0.29098057746887207, 0.3711062967777252, 0.3652859628200531, 0.3358672261238098, 0.39771372079849243, 0.34906265139579773, 0.22855718433856964, 0.4383459687232971, 0.37471190094947815, 0.37336182594299316, 0.27942797541618347, 0.4381335973739624, 0.40638232231140137, 0.4177491366863251, 0.47006016969680786, 0.48085469007492065, 0.4943237006664276, 0.41431567072868347, 0.23962372541427612, 0.23859629034996033, 0.5081983804702759, 0.4303555190563202, 0.24042411148548126, 0.28214991092681885, 0.34304240345954895, 0.46261653304100037, 0.38718122243881226, 0.34329119324684143, 0.11259938776493073, 0.4044314920902252, 0.4390060603618622, 0.43350619077682495, 0.22228723764419556, 0.3711062967777252, 0.192385733127594, 0.38068780303001404, 0.4711713492870331, 0.46261653304100037, 0.4676802456378937, 0.192385733127594, 0.48400798439979553, 0.21479517221450806, 0.46161240339279175, 0.4519442319869995, 0.36966821551322937, 0.3349739909172058, 0.517052412033081, 0.17007610201835632, 0.5095820426940918], [0.2503466010093689, 0.2020440250635147, 0.26149317622184753, 0.24977323412895203, 0.46947357058525085, 0.2757308781147003, 0.2515537440776825, 0.21062709391117096, 0.2113783359527588, 0.24784378707408905, 0.26908063888549805, 0.27427583932876587, 0.24978451430797577, 0.40153729915618896, 0.2939232885837555, 0.2656635344028473, 0.21236886084079742, 0.46488144993782043, 0.26131778955459595, 0.23589013516902924, 0.32946985960006714, 0.22696736454963684, 0.48348984122276306, 0.30887019634246826, 0.32216933369636536, 0.3576240837574005, 0.30894985795021057, 0.26721954345703125, 0.4334700107574463, 0.3616887331008911, 0.2646574378013611, 0.2117232084274292, 0.2125645875930786, 0.22234351933002472, 0.23867446184158325, 0.3042781949043274, 0.28362908959388733, 0.3890799880027771, 0.23055973649024963, 0.44242894649505615, 0.4363916218280792, 0.47294723987579346, 0.23661553859710693, 0.24142786860466003, 0.23991483449935913, 0.3737531006336212, 0.19598683714866638, 0.22547663748264313, 0.24273313581943512, 0.230179563164711, 0.25993797183036804, 0.23565953969955444, 0.24535778164863586, 0.20740129053592682, 0.30066779255867004, 0.28745323419570923, 0.2524801194667816, 0.21675363183021545, 0.32938525080680847, 0.3323187232017517, 0.2117232084274292, 0.3644196093082428, 0.1899951547384262, 0.21214622259140015, 0.2659801244735718, 0.3689897358417511, 0.2792246341705322, 0.28745323419570923, 0.2869616746902466, 0.26783519983291626, 0.3069818615913391, 0.24292048811912537, 0.21327592432498932, 0.2938843369483948, 0.2634509801864624, 0.2048262506723404, 0.21936093270778656, 0.22691792249679565, 0.27490007877349854, 0.23879529535770416, 0.22065110504627228, 0.2184896618127823, 0.42558538913726807, 0.22010783851146698, 0.2587617039680481, 0.3016816973686218, 0.2117232084274292, 0.24641723930835724, 0.31237122416496277, 0.20533692836761475, 0.2048262506723404, 0.23285101354122162, 0.2848964035511017, 0.25557923316955566, 0.20732833445072174, 0.22378531098365784, 0.2659801244735718, 0.32118716835975647, 0.20248177647590637, 0.3320939838886261, 0.22580240666866302, 0.2495141178369522, 0.2101420909166336, 0.35057976841926575, 0.23879529535770416, 0.38563355803489685, 0.2465054988861084, 0.22010783851146698, 0.21767063438892365, 0.2612795829772949, 0.2938843369483948, 0.21461574733257294, 0.2972410023212433, 0.3158494830131531, 0.19072650372982025, 0.30108964443206787, 0.25596004724502563, 0.3059921860694885, 0.2355772852897644, 0.30158066749572754, 0.25166386365890503, 0.32778456807136536, 0.21878495812416077, 0.2486429065465927, 0.3531777858734131, 0.24292445182800293, 0.24353821575641632, 0.3744182288646698, 0.2058965414762497, 0.49192607402801514, 0.27847054600715637, 0.3801195025444031, 0.4307769238948822, 0.291231632232666, 0.22038507461547852, 0.2364458441734314, 0.27264323830604553, 0.24822627007961273, 0.2139044553041458, 0.23374377191066742, 0.3138287365436554, 0.2511851489543915, 0.2771008610725403, 0.26540327072143555, 0.3128119111061096, 0.207791268825531, 0.3431291878223419, 0.23006963729858398, 0.25410178303718567, 0.34863388538360596, 0.35873115062713623, 0.2518453598022461, 0.2701578736305237, 0.2101420909166336, 0.21865645051002502, 0.27104464173316956, 0.2332894653081894, 0.35057976841926575, 0.3962671458721161, 0.23220783472061157, 0.475594162940979, 0.331294447183609, 0.5174918174743652, 0.21878495812416077, 0.2230164110660553, 0.2516457736492157, 0.479449063539505, 0.38919880986213684, 0.26285508275032043, 0.30342456698417664, 0.30095210671424866, 0.24986803531646729, 0.19364190101623535, 0.31041496992111206, 0.20670127868652344, 0.4337628483772278, 0.2158581167459488, 0.4097353518009186, 0.2085237205028534, 0.30978891253471375, 0.2662716507911682, 0.28959596157073975, 0.25972995162010193, 0.3115253746509552, 0.25059136748313904, 0.22267688810825348, 0.22333569824695587, 0.3724152743816376, 0.291231632232666, 0.2548898160457611, 0.30688852071762085, 0.36704662442207336, 0.25621360540390015, 0.35873115062713623, 0.21715755760669708, 0.2120114266872406, 0.21427784860134125, 0.29682961106300354, 0.25707748532295227, 0.29543545842170715, 0.25614404678344727, 0.22709451615810394, 0.20220009982585907, 0.21327592432498932, 0.2051580548286438, 0.21274107694625854, 0.2976468801498413, 0.3418461084365845, 0.2753264605998993, 0.3911155164241791, 0.22277337312698364, 0.3702513873577118, 0.42902517318725586, 0.21550506353378296, 0.2154625654220581, 0.21476253867149353, 0.20942756533622742, 0.32283923029899597, 0.2263897955417633, 0.3858567774295807, 0.3240016996860504, 0.2949182093143463, 0.2727500796318054, 0.3205014765262604, 0.32433825731277466, 0.25152796506881714, 0.20947714149951935, 0.29375946521759033, 0.23386767506599426, 0.18511416018009186, 0.2936900854110718, 0.3109607994556427, 0.22333569824695587, 0.25059136748313904, 0.38563355803489685, 0.2641405761241913, 0.2154625654220581, 0.4536456763744354, 0.31984639167785645, 0.23547042906284332, 0.3357321619987488, 0.39840686321258545, 0.2184896618127823, 0.326080858707428, 0.29869958758354187, 0.2313157618045807, 0.23609767854213715, 0.2750835418701172, 0.2907893657684326, 0.22207936644554138, 0.24972550570964813, 0.21087761223316193, 0.26285508275032043, 0.22125591337680817, 0.1909862607717514, 0.3922592103481293, 0.20785953104496002, 0.31233900785446167, 0.27663514018058777, 0.22995446622371674, 0.27162182331085205, 0.26785778999328613, 0.2919669449329376, 0.3382916748523712, 0.21475283801555634, 0.24884763360023499, 0.3680795729160309, 0.27847054600715637, 0.348806768655777, 0.3962671458721161, 0.26151254773139954, 0.3111706078052521, 0.26540327072143555, 0.22907917201519012, 0.4749622642993927, 0.2771008610725403, 0.19364190101623535, 0.42859166860580444, 0.1951448619365692, 0.2517356872558594, 0.27179330587387085, 0.21069835126399994, 0.25367507338523865, 0.2713245153427124, 0.29438942670822144, 0.23298273980617523, 0.1937897801399231, 0.2350696623325348, 0.24259990453720093, 0.28932178020477295, 0.26907825469970703, 0.2314896583557129, 0.30980175733566284, 0.22362513840198517, 0.25113940238952637, 0.28684109449386597, 0.2640889286994934, 0.3334699869155884, 0.22666433453559875, 0.2640889286994934, 0.24246931076049805, 0.21771015226840973, 0.32827484607696533, 0.25312110781669617, 0.19887489080429077, 0.2192266583442688, 0.3384873867034912, 0.29562538862228394, 0.2756558060646057, 0.4643346965312958, 0.39567577838897705, 0.261711061000824, 0.3037327527999878, 0.23393993079662323, 0.21647687256336212, 0.23953939974308014, 0.31048595905303955, 0.22102461755275726, 0.3408724069595337, 0.27388277649879456, 0.21814607083797455, 0.23822109401226044, 0.23304936289787292, 0.3855876624584198, 0.2636782228946686, 0.2653712332248688, 0.25113940238952637, 0.2337735891342163, 0.22514155507087708, 0.23607517778873444, 0.303094744682312, 0.39567577838897705, 0.25367507338523865, 0.29837465286254883, 0.20705647766590118, 0.2602072060108185, 0.25458642840385437, 0.2051870971918106, 0.2518393099308014, 0.26582950353622437, 0.23067942261695862, 0.5472932457923889, 0.23410548269748688, 0.261711061000824, 0.22503983974456787, 0.21848922967910767, 0.21819525957107544, 0.27455398440361023, 0.22701579332351685, 0.23014453053474426, 0.512687623500824, 0.2392669916152954, 0.30779311060905457, 0.21589884161949158, 0.2780604362487793, 0.2725149691104889, 0.2267398089170456, 0.43303969502449036, 0.2889339029788971, 0.3155158758163452, 0.28932178020477295, 0.2711954712867737, 0.4643346965312958, 0.2774386405944824, 0.2646417021751404, 0.5405666828155518, 0.26456210017204285, 0.23779545724391937, 0.21436409652233124, 0.27539563179016113, 0.2518393099308014, 0.21814607083797455, 0.24511268734931946, 0.3408724069595337, 0.2889339029788971, 0.22826631367206573, 0.22826631367206573, 0.3163861334323883, 0.32144105434417725, 0.18156763911247253, 0.3855876624584198, 0.303094744682312, 0.22102461755275726, 0.2192150503396988, 0.3334699869155884, 0.3382766544818878, 0.2629929780960083, 0.2314896583557129, 0.27455398440361023, 0.2522207796573639, 0.2171759158372879, 0.3827337622642517, 0.26907825469970703, 0.21643370389938354, 0.2896472215652466, 0.22740432620048523, 0.23447449505329132, 0.3541293144226074, 0.2629929780960083, 0.43303969502449036, 0.21656468510627747, 0.32144105434417725, 0.21624848246574402, 0.2074989527463913, 0.40729498863220215, 0.27501553297042847, 0.23607517778873444, 0.24511268734931946, 0.4149542450904846, 0.23566237092018127, 0.27390655875205994, 0.2051870971918106, 0.22333569824695587, 0.29574859142303467, 0.22740432620048523, 0.23510302603244781, 0.31048595905303955, 0.24080036580562592, 0.2012796700000763, 0.21589884161949158, 0.26459571719169617, 0.23750436305999756, 0.4783797860145569, 0.2267398089170456, 0.2108604609966278, 0.33585771918296814, 0.29185864329338074, 0.26656484603881836, 0.23492945730686188, 0.27431997656822205, 0.2653712332248688, 0.2646417021751404, 0.2973971366882324, 0.2613605260848999, 0.23447449505329132, 0.21436409652233124, 0.3181751072406769, 0.24929969012737274, 0.4314100742340088, 0.27388277649879456, 0.2722022235393524, 0.38358747959136963, 0.37415003776550293, 0.23750436305999756, 0.512687623500824, 0.22805172204971313, 0.24246931076049805, 0.4744444191455841, 0.28014248609542847, 0.23304936289787292, 0.26656484603881836, 0.23392896354198456, 0.25781604647636414, 0.25236591696739197, 0.2337735891342163, 0.2713245153427124, 0.250078409910202, 0.20674251019954681, 0.2835988402366638, 0.26582950353622437, 0.24041610956192017, 0.21014639735221863, 0.29562538862228394, 0.3033420443534851, 0.25458642840385437, 0.24929969012737274, 0.41685107350349426, 0.44933322072029114, 0.3541293144226074, 0.22805172204971313, 0.22333569824695587, 0.4802495241165161, 0.27992263436317444, 0.21430718898773193, 0.22790805995464325, 0.27431997656822205, 0.34404507279396057, 0.27501553297042847, 0.2520020008087158, 0.20544931292533875, 0.2636782228946686, 0.3598793148994446, 0.30579817295074463, 0.22946563363075256, 0.25781604647636414, 0.1923292726278305, 0.26542168855667114, 0.2756558060646057, 0.34404507279396057, 0.2973971366882324, 0.1923292726278305, 0.38358747959136963, 0.2024928778409958, 0.30980175733566284, 0.2841144800186157, 0.3033696711063385, 0.24669572710990906, 0.4744444191455841, 0.21268093585968018, 0.43115049600601196], [0.33406156301498413, 0.19039210677146912, 0.3155946731567383, 0.36284512281417847, 0.4656297266483307, 0.3229149878025055, 0.3173423111438751, 0.19808611273765564, 0.12849044799804688, 0.21740594506263733, 0.3748859465122223, 0.3392043709754944, 0.27233999967575073, 0.4754270613193512, 0.34720879793167114, 0.365565687417984, 0.16502653062343597, 0.5313615798950195, 0.35516199469566345, 0.2394007295370102, 0.3254995048046112, 0.21450838446617126, 0.5238208174705505, 0.39523226022720337, 0.49622076749801636, 0.4187289774417877, 0.47632139921188354, 0.37983307242393494, 0.48630115389823914, 0.4822733998298645, 0.34490424394607544, 0.15706422924995422, 0.17366954684257507, 0.2280312180519104, 0.2427801936864853, 0.3895375430583954, 0.4084089398384094, 0.49785879254341125, 0.24300983548164368, 0.5828236937522888, 0.5389032363891602, 0.5657512545585632, 0.24785499274730682, 0.3074391186237335, 0.30893945693969727, 0.5463468432426453, 0.15527644753456116, 0.2087269425392151, 0.35114428400993347, 0.21183985471725464, 0.33555394411087036, 0.3007916212081909, 0.27147525548934937, 0.17753353714942932, 0.43020445108413696, 0.3941769599914551, 0.35131341218948364, 0.19581414759159088, 0.4399580955505371, 0.46387779712677, 0.15706422924995422, 0.46374720335006714, 0.14123889803886414, 0.2247578501701355, 0.341193825006485, 0.5189352631568909, 0.34785494208335876, 0.3941769599914551, 0.36607223749160767, 0.36795586347579956, 0.4446598291397095, 0.2906883955001831, 0.2773727476596832, 0.36533123254776, 0.3555251359939575, 0.155247300863266, 0.19580528140068054, 0.30161166191101074, 0.2742639482021332, 0.2019071727991104, 0.1579921841621399, 0.15426678955554962, 0.4952085316181183, 0.2543220818042755, 0.32041651010513306, 0.42409297823905945, 0.15706422924995422, 0.28822222352027893, 0.4636319577693939, 0.14000678062438965, 0.155247300863266, 0.2508257031440735, 0.4427688717842102, 0.30033475160598755, 0.15317589044570923, 0.24868057668209076, 0.341193825006485, 0.4549112021923065, 0.14476735889911652, 0.4947310984134674, 0.23224690556526184, 0.1921558529138565, 0.19928023219108582, 0.4739495813846588, 0.2019071727991104, 0.4311673045158386, 0.20250581204891205, 0.2543220818042755, 0.14844214916229248, 0.3288293480873108, 0.36533123254776, 0.18375614285469055, 0.3838255703449249, 0.37394726276397705, 0.16943466663360596, 0.4377540349960327, 0.3113676905632019, 0.4343513548374176, 0.29950812458992004, 0.42038795351982117, 0.27862271666526794, 0.4834415018558502, 0.17443406581878662, 0.30327126383781433, 0.5036317110061646, 0.3210262358188629, 0.27641475200653076, 0.441355437040329, 0.16063983738422394, 0.5783546566963196, 0.18659119307994843, 0.4677848517894745, 0.5344792008399963, 0.3101848363876343, 0.21608544886112213, 0.2639688551425934, 0.3490760028362274, 0.2730798125267029, 0.19044774770736694, 0.3579103350639343, 0.46387720108032227, 0.2732032537460327, 0.30845654010772705, 0.3493693470954895, 0.36241036653518677, 0.1723710000514984, 0.47765177488327026, 0.2714609205722809, 0.29266875982284546, 0.5297683477401733, 0.4807031750679016, 0.1971837878227234, 0.3051547408103943, 0.19928023219108582, 0.1614348441362381, 0.34576869010925293, 0.2471044808626175, 0.4739495813846588, 0.5240857601165771, 0.35828617215156555, 0.5414401888847351, 0.44394710659980774, 0.6377803683280945, 0.17443406581878662, 0.18964877724647522, 0.3477552831172943, 0.5847934484481812, 0.5013059973716736, 0.25326383113861084, 0.46493402123451233, 0.4268951714038849, 0.3209937810897827, 0.16903962194919586, 0.38535571098327637, 0.130096897482872, 0.5031962394714355, 0.20506419241428375, 0.48672303557395935, 0.20345881581306458, 0.46467429399490356, 0.305841326713562, 0.4388650357723236, 0.26862555742263794, 0.43734613060951233, 0.25474822521209717, 0.21073250472545624, 0.17897319793701172, 0.47850897908210754, 0.3101848363876343, 0.3684498369693756, 0.3914695978164673, 0.4893490970134735, 0.40128272771835327, 0.4807031750679016, 0.25186651945114136, 0.16144472360610962, 0.21165308356285095, 0.4030914604663849, 0.44823673367500305, 0.36004310846328735, 0.29961857199668884, 0.20123286545276642, 0.15996158123016357, 0.2773727476596832, 0.22215434908866882, 0.17818798124790192, 0.28533220291137695, 0.48959723114967346, 0.3390918970108032, 0.49366965889930725, 0.22085943818092346, 0.5012709498405457, 0.5277305841445923, 0.15772368013858795, 0.20948819816112518, 0.24493210017681122, 0.17287516593933105, 0.4285948574542999, 0.27876871824264526, 0.45375218987464905, 0.4158026874065399, 0.3846289813518524, 0.40182384848594666, 0.4011155962944031, 0.3782083988189697, 0.21597112715244293, 0.24189595878124237, 0.35361239314079285, 0.2857895791530609, 0.12927329540252686, 0.43624722957611084, 0.48003023862838745, 0.17897319793701172, 0.25474822521209717, 0.4311673045158386, 0.3827522099018097, 0.20948819816112518, 0.5798969864845276, 0.4251740574836731, 0.242191880941391, 0.505072295665741, 0.5176897048950195, 0.15426678955554962, 0.5339750647544861, 0.4083789885044098, 0.2356540560722351, 0.27110180258750916, 0.43421491980552673, 0.36563533544540405, 0.1788550317287445, 0.24587373435497284, 0.17373552918434143, 0.25326383113861084, 0.23987111449241638, 0.16689050197601318, 0.5538660883903503, 0.15103109180927277, 0.4581665098667145, 0.3234676420688629, 0.20712384581565857, 0.3738406002521515, 0.25640180706977844, 0.3579782545566559, 0.43893420696258545, 0.20199915766716003, 0.22543776035308838, 0.5152961611747742, 0.18659119307994843, 0.5032336711883545, 0.5240857601165771, 0.47211071848869324, 0.4532310366630554, 0.3493693470954895, 0.20386774837970734, 0.5699548721313477, 0.30845654010772705, 0.16903962194919586, 0.5243255496025085, 0.14236563444137573, 0.4500081539154053, 0.3995615541934967, 0.16867832839488983, 0.36445626616477966, 0.36122360825538635, 0.4587548077106476, 0.2665848731994629, 0.15134355425834656, 0.24050116539001465, 0.2721008360385895, 0.4482175409793854, 0.39644062519073486, 0.2734183967113495, 0.46767255663871765, 0.23193873465061188, 0.375116765499115, 0.33278003334999084, 0.23845535516738892, 0.34643277525901794, 0.20673741400241852, 0.23845535516738892, 0.2993958592414856, 0.23614639043807983, 0.41208896040916443, 0.32308489084243774, 0.1514463573694229, 0.21719972789287567, 0.5058112740516663, 0.4520670175552368, 0.4065179228782654, 0.5068744421005249, 0.48466911911964417, 0.20112523436546326, 0.44714292883872986, 0.201554536819458, 0.19193333387374878, 0.27720731496810913, 0.500516951084137, 0.23462699353694916, 0.5161162614822388, 0.32270151376724243, 0.20124536752700806, 0.2753743827342987, 0.23542232811450958, 0.5304440259933472, 0.3499596416950226, 0.3739515542984009, 0.375116765499115, 0.29406315088272095, 0.2593502700328827, 0.20128192007541656, 0.39579132199287415, 0.48466911911964417, 0.36445626616477966, 0.42563191056251526, 0.14850422739982605, 0.2928364872932434, 0.35213503241539, 0.19208362698554993, 0.3058137595653534, 0.31015005707740784, 0.21357084810733795, 0.6529402732849121, 0.2935989201068878, 0.20112523436546326, 0.2733239233493805, 0.1795247197151184, 0.16861918568611145, 0.3380294442176819, 0.22110982239246368, 0.22208468616008759, 0.5578460097312927, 0.22818998992443085, 0.44778987765312195, 0.171551913022995, 0.3668918013572693, 0.3118256628513336, 0.2124435156583786, 0.53424471616745, 0.4494655132293701, 0.24941182136535645, 0.4482175409793854, 0.37338343262672424, 0.5068744421005249, 0.36381295323371887, 0.37801840901374817, 0.5661818385124207, 0.44415268301963806, 0.25907155871391296, 0.1809934675693512, 0.3970032334327698, 0.3058137595653534, 0.20124536752700806, 0.2613663673400879, 0.5161162614822388, 0.4494655132293701, 0.20264270901679993, 0.20264270901679993, 0.39343541860580444, 0.4506278336048126, 0.13767006993293762, 0.5304440259933472, 0.39579132199287415, 0.23462699353694916, 0.18945178389549255, 0.34643277525901794, 0.47106197476387024, 0.3860156834125519, 0.2734183967113495, 0.3380294442176819, 0.33838599920272827, 0.17019502818584442, 0.507466733455658, 0.39644062519073486, 0.1951361894607544, 0.3722052574157715, 0.16006773710250854, 0.19372661411762238, 0.3572414815425873, 0.3860156834125519, 0.53424471616745, 0.23172403872013092, 0.4506278336048126, 0.24712738394737244, 0.1584189236164093, 0.49101585149765015, 0.30303874611854553, 0.20128192007541656, 0.2613663673400879, 0.5085708498954773, 0.27318301796913147, 0.32814300060272217, 0.19208362698554993, 0.17897319793701172, 0.43546703457832336, 0.16006773710250854, 0.22336457669734955, 0.500516951084137, 0.19960467517375946, 0.17744849622249603, 0.171551913022995, 0.3911468982696533, 0.20813611149787903, 0.5648012161254883, 0.2124435156583786, 0.17177246510982513, 0.4930512011051178, 0.3753756582736969, 0.30709514021873474, 0.16855044662952423, 0.3881120979785919, 0.3739515542984009, 0.37801840901374817, 0.48925307393074036, 0.36090025305747986, 0.19372661411762238, 0.1809934675693512, 0.40546488761901855, 0.34482502937316895, 0.5416842699050903, 0.32270151376724243, 0.41439664363861084, 0.5216248631477356, 0.4898034632205963, 0.20813611149787903, 0.5578460097312927, 0.2299717217683792, 0.2993958592414856, 0.5612612366676331, 0.4091074466705322, 0.23542232811450958, 0.30709514021873474, 0.2905420958995819, 0.39097094535827637, 0.3736755847930908, 0.29406315088272095, 0.36122360825538635, 0.3346790671348572, 0.1839967966079712, 0.4037468433380127, 0.31015005707740784, 0.256519615650177, 0.18958169221878052, 0.4520670175552368, 0.22997301816940308, 0.35213503241539, 0.34482502937316895, 0.4708500802516937, 0.5519422292709351, 0.3572414815425873, 0.2299717217683792, 0.17897319793701172, 0.5650361776351929, 0.4498136043548584, 0.1743835061788559, 0.2909211814403534, 0.3881120979785919, 0.490559846162796, 0.30303874611854553, 0.266584187746048, 0.15333342552185059, 0.3499596416950226, 0.42763352394104004, 0.344703733921051, 0.16929461061954498, 0.39097094535827637, 0.16852672398090363, 0.3455311357975006, 0.4065179228782654, 0.490559846162796, 0.48925307393074036, 0.16852672398090363, 0.5216248631477356, 0.16683204472064972, 0.46767255663871765, 0.47013282775878906, 0.3557654321193695, 0.2936829924583435, 0.5612612366676331, 0.15685179829597473, 0.5042648911476135]]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(SAVE_PATH,map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-17T09:32:26.048763Z",
     "iopub.execute_input": "2023-12-17T09:32:26.049736Z",
     "iopub.status.idle": "2023-12-17T09:32:26.584887Z",
     "shell.execute_reply.started": "2023-12-17T09:32:26.049694Z",
     "shell.execute_reply": "2023-12-17T09:32:26.583889Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
